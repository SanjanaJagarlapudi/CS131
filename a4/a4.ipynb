{"cells":[{"cell_type":"code","execution_count":1,"id":"b5644bd9","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["25/04/25 19:20:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|\n","+---------------+------------+------------+------------+\n","|            1.0|       151.0|       239.0|        9.95|\n","|            1.0|       239.0|       246.0|        16.3|\n","|            3.0|       236.0|       236.0|         5.8|\n","|            5.0|       193.0|       193.0|        7.55|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            5.0|       193.0|       193.0|       13.31|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            1.0|       163.0|       229.0|        9.05|\n","|            1.0|       229.0|         7.0|        18.5|\n","|            2.0|       141.0|       234.0|        13.0|\n","+---------------+------------+------------+------------+\n","only showing top 10 rows\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","\n","\n","\n","# Create a SparkSession instance (an entry point to all Spark functions)\n","spark = SparkSession.builder.appName(\"a4\").getOrCreate()\n","\n","\n"]},{"cell_type":"code","execution_count":33,"id":"b10e151c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|\n","+---------------+------------+------------+------------+\n","|            1.0|       151.0|       239.0|        9.95|\n","|            1.0|       239.0|       246.0|        16.3|\n","|            3.0|       236.0|       236.0|         5.8|\n","|            5.0|       193.0|       193.0|        7.55|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            5.0|       193.0|       193.0|       13.31|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            1.0|       163.0|       229.0|        9.05|\n","|            1.0|       229.0|         7.0|        18.5|\n","|            2.0|       141.0|       234.0|        13.0|\n","+---------------+------------+------------+------------+\n","only showing top 10 rows\n","\n"]}],"source":["df = spark.read.csv('gs://dataproc-staging-us-central1-721720945833-6cwqbnms/2019-01-h1.csv', header=True, inferSchema=True)\n","# Only care about these rows\n","filtered = df.select(\"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\")\n","\n","filtered.show(10)"]},{"cell_type":"code","execution_count":34,"id":"d434fa11","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 247:==========================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["2920849 730150\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# Splitting into test and train \n","train_df, test_df = filtered.randomSplit([.8, .2], seed=42)\n","print(train_df.count(), test_df.count())\n"]},{"cell_type":"code","execution_count":48,"id":"2fb32cda","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 369:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+---------------+------------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|       features|        prediction|\n","+---------------+------------+------------+------------+---------------+------------------+\n","|            0.0|         4.0|         4.0|         4.3|  [0.0,4.0,4.0]|23.931810506566894|\n","|            0.0|         4.0|        33.0|       17.75| [0.0,4.0,33.0]|19.266741166043648|\n","|            0.0|         4.0|        68.0|        15.8| [0.0,4.0,68.0]| 18.39775119715606|\n","|            0.0|         4.0|        79.0|        9.75| [0.0,4.0,79.0]| 18.39775119715606|\n","|            0.0|         4.0|       125.0|         9.3|[0.0,4.0,125.0]| 18.39775119715606|\n","|            0.0|         4.0|       170.0|       11.15|[0.0,4.0,170.0]| 18.39775119715606|\n","|            0.0|         7.0|         7.0|        0.31|  [0.0,7.0,7.0]|19.266741166043648|\n","|            0.0|         7.0|         7.0|         6.3|  [0.0,7.0,7.0]|19.266741166043648|\n","|            0.0|         7.0|       112.0|        16.8|[0.0,7.0,112.0]| 18.39775119715606|\n","|            0.0|         7.0|       138.0|        10.8|[0.0,7.0,138.0]| 18.39775119715606|\n","+---------------+------------+------------+------------+---------------+------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import DecisionTreeRegressor\n","from pyspark.ml import Pipeline\n","\n","# First, we have to make a vector assembler, so that spark can actually read the columns\n","assembler = VectorAssembler(\n","    inputCols=[\"passenger_count\", \"pulocationid\", \"dolocationid\"],\n","    outputCol=\"features\"\n",")\n","\n","\n","#next, we make the regressor.\n","dt = DecisionTreeRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"total_amount\",\n","    maxBins=200\n",")\n","\n","#to make things easier, we can make a pipeline that does the things above\n","pipeline = Pipeline(stages=[assembler, dt])\n","\n","#fit the model on the training data set.\n","pipelineModel = pipeline.fit(train_df)\n","\n","predDf = pipelineModel.transform(test_df)\n","\n","predDf.show(10)\n"]},{"cell_type":"code","execution_count":49,"id":"61188ed6","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+---------------+------------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|       features|        prediction|\n","+---------------+------------+------------+------------+---------------+------------------+\n","|            0.0|         1.0|         1.0|      116.75|  [0.0,1.0,1.0]|23.931810506566894|\n","|            0.0|         4.0|        17.0|        20.3| [0.0,4.0,17.0]|19.266741166043648|\n","|            0.0|         4.0|        68.0|        12.8| [0.0,4.0,68.0]| 18.39775119715606|\n","|            0.0|         4.0|        79.0|        6.35| [0.0,4.0,79.0]| 18.39775119715606|\n","|            0.0|         4.0|        90.0|        15.8| [0.0,4.0,90.0]| 18.39775119715606|\n","|            0.0|         4.0|       125.0|       13.55|[0.0,4.0,125.0]| 18.39775119715606|\n","|            0.0|         4.0|       170.0|        14.3|[0.0,4.0,170.0]| 18.39775119715606|\n","|            0.0|         4.0|       264.0|         8.3|[0.0,4.0,264.0]| 18.39775119715606|\n","|            0.0|         7.0|         7.0|         7.3|  [0.0,7.0,7.0]|19.266741166043648|\n","|            0.0|         7.0|         7.0|         7.8|  [0.0,7.0,7.0]|19.266741166043648|\n","+---------------+------------+------------+------------+---------------+------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 371:==========================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["(RMSE) on test data = 60.13\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.ml.evaluation import RegressionEvaluator\n","\n","#Showing predictions alongside the original features\n","predDf.show(10)\n","\n","#Evaluat the model using RMSE\n","evaluator = RegressionEvaluator(\n","    labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\"\n",")\n","\n","RMSE = evaluator.evaluate(predDf)\n","print(f\"(RMSE) on test data = {RMSE:.2f}\")"]},{"cell_type":"code","execution_count":null,"id":"b932d8d1","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d2bbf9a6","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}